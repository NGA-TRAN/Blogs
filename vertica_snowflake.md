# Distributed Cloud Columnar Databases: Vertica Eon vs Snowflake

*February 2021*

## Introduction

This blog compares the two successful Databases, __Vertica__ and __Snowflake__, focusing on their major offers and internal implementations. The main comparisons are based on these technical materials, [Vertica](https://vldb.org/pvldb/vol5/p1790_andrewlamb_vldb2012.pdf), 
[Vertica Eon](https://www.vertica.com/wp-content/uploads/2018/05/Vertica_EON_SIGMOD_Paper.pdf), [Vertica Eon Talk 2020](https://www.thecube.net/vertica-bigdata-2020/content/Videos/GGE42drgkAHfYoFbn), 
and [Snowflake](https://pages.cs.wisc.edu/~remzi/Classes/739/Fall2018/Papers/p215-dageville-snowflake.pdf), [Snowflake Architecture Talk 2019](https://www.youtube.com/watch?v=dxrEHqMFUWI), [Snowflake Talk at CIDR 2021](https://www.youtube.com/watch?v=0K7h7WvC6D4) 
 but many other related ones will be mentioned or cited throughout the writing. The content of this blog is from my sole understanding of the materials and may miss some key designs. All feedback to [this contact](https://github.com/NGA-TRAN/Blogs/blob/main/README.md) is welcome and appreciated.

*__In short, being a Distributed Cloud RDBMS<sup>(1)</sup> that is able to scale their compute up (use more powerful nodes) and out (add more nodes) while maintaining strong consistency and availability, Vertica's and Snowflake's offers are very unique in the market.__* Let us dig into the details.

## Similarities

Vertica and Snowflake provide similar _Cloud Databases_ that support separation of _storage_ and _compute_ in which their data and metadata are shared and stored persistently on a durable and available storage such as Amazon S3, and their compute clusters are formed elastically on demand to write and read data to and from the shared storage. 

Their Cloud DBs are _Distributed RDBMS's_ running _SQL_ on an MPP (Massively Parallel Processing) that complies with _ACID_ (Atomic Consistent Isolated Durable) transactions, allows _fault tolerance_, _self-recovers_, and _self-backups_. Although both databases work well on writing and reading data, they are referred as Data Warehouses because of their focus on _OLAP_ (Online Analytical Processing) where read happens a lot more often than write; and therefore leverage _Column Store_ <sup>(*1)</sup> technologies in their table storage designs.

The two databases are designed to support _historical queries_ that can access deleted data <sup>(*2)</sup>. This is possible because data files are immutable after they are first written. A DELETE is just a new write to store the positions of deleting tuples, and at SELECT time, that brief information is used to eliminate deleted rows before returning results to users. An UPDATE is a DELETE followed by an INSERT. Moreover, they provide zero-copy clones of tables SQL <sup>(*3)</sup> that duplicate the table data for users without copying anything. This is possible because the persistent data is immutable. Both old and new tables use the same files of data before the clone time, and the data modifications after that are managed by each table metadata separately. 

Both companies see the popularity of _semi-structured_ data (e.g. JSON, XML, and AVRO) that need non-traditional workload, ELT (Extract, Load, and Transform)<sup>(2)</sup>, and offer features to load semi-structured data first and, when needed, discover and transform the structure part of the semi-structured data automatically or through UDF (User Defined Functions) for better query performance <sup>(*4)</sup>. 

As Cloud DB providers, they continue building and improving their data _security_ features such as authentication, authorization, and encryption in flight and at rest <sup>(*5) and ($1)</sup>.

The ecosystem of both DBs are very diverse including integration with Kafka for data streaming <sup>(*6)</sup>, Spark connector for bringing data between their DBs and Spark ecosystem <sup>(*7)</sup>, SDK for different programming languages <sup>(*8)</sup>, driver connectors <sup>(*9)</sup>, and BI integration <sup>(*10)</sup>. I will not dive into their details but these two talks of [Snowflake's Best Practice for Data Engineers](https://www.youtube.com/watch?v=E--zIGd_iqE&feature=youtu.be) and [Vertica's Coolest Features](https://www.youtube.com/watch?v=ezD5we-cens&feature=emb_logo) summarize well those offers.

Here are a few key technologies of their internal implementations:
* ACID transactions are handled via snapshot isolation and MVCC (multi-version concurrency control) in which queries in a transaction only see a snapshot of immutable files of data at the time the transaction started and a copy of every change of metadata or data is preserved for some duration after their commit.
* Columns are not only stored separately in different files but also sorted and encoded<sup>(3)</sup> which are keys for fast scan and efficient query execution. Furthermore, by partitioning tables horizontally and keeping column partition min and max values, their execution engines know to only scan partitions that include needed data. 
* Query plans are built and filtered by a cost-based Query Optimizer using statistics of the data<sup>(*11)</sup>.
* Queries are executed in vectorized and pipelined fashion to improve cache efficiency and avoid storing intermediate results.
* Even though most optimizations are performed in Query Optimizer, a few are decided on-the-fly at execution time. 
* Join SIP (Sideway Information Passing) techniques are implemented to prune unnecessary join data at scan steps, way before the join actually happens<sup>(*12)</sup>.
* Execution counters are logged for every executed operator for comparing with their corresponding estimated values.

## Differences

While Vertica offers both _Cloud_ DB (Eon mode) and _On-Premise_ DB (Enterprise mode), Snowflake is a _DBaaS_ (Database as a Service) and only offers the _Cloud_ option. Snowflake focuses on **easy-to-use** and makes everything simple for users. Users pick their general needs and Snowflake will provide suitable configurations for them. Vertica focuses on **query performance** and lets users optimize their clusters and storage as needed. Each detailed comparison below goes with [S] or [V] to emphasize who is more mature, Snowflake or Vertica respectively. 

[S] As a _**DBaaS**_, Snowflake provides a pretty and friendly UI for almost all actions including setting up clusters, loading data, running queries, monitoring on-going processes and reporting results.<sup>(#1)</sup> The UI is also expended for user collaboration, providing feedback and getting support. At the time of this writing, Vertica is not yet a DBaaS but does provide self-management service such as [ElasticDW](https://elasticdw.com) and work well on different UI such as DBVisualizer and its own Management Console to provide users some nice experience<sup>(#2)</sup>.

[S] Snowflake's architecture includes three layers: _**operation service**_ (cloud service), _**execution compute**_ (VW or Virtual Warehouse), and _**storage**_, which I think a very smart design to divide and conquer the three major components of an _elastic and multi-tenant cluster_: operation, execution, and storage.  The operation includes metadata & transaction management, cluster infrastructure management, query optimizer, and security. Vertica Cloud DB was evolved from its On-Premise one and implements two layers: _**operation & execution**_  and _**storage**_. Even though coupling operation and execution works best for a distributed shared nothing on-premise system, a center-point to manage the whole cluster is needed for a cloud DB. Hence, Vertica has made a very quick design to use one of its sub-clusters (equivalent to Snowflake's VW) as a primary one to not only run queries as other sub-clusters but also responsible for the whole cluster operation. I think this is a great turn-around architecture to serve as an intermediate path for Vertica to completely split its operation & execution to support more cloud-oriented services. To my best knowledge of Vertica internal implementations, the principle designs of its operation and execution are not tightly coupled and it won't take much effort to split them.

Caching is quite interesting in both:
* [S] At the operation service layer, Snowflake maintains a _hot cache_ that captures query _results_ for a configurable period but usually 24 hours. If the same or different users run that same query again and its data has not been modified since its last run, its cached results will be returned immediately without running the query. This feature is useful in a collaborative workplace where many users look for the same reports<sup>(*13) and (#3)</sup>.
* [V] At the execution compute layer, both Snowflake and Vertica offer _warm cache_ to store data on the compute local disks to avoid reading data from their slow object stores such as S3. In Snowflake, this cache is hidden from users, only captures data of recently run queries, and disappears when that compute is suspended. In Vertica, this cache is called _**depot**_ whose size, caching policies, and other configurations such as whether it will persist even after the compute are shutdown, can be specified by users<sup>(*14)</sup>. A node with empty depot can accept queries but the depot is usually filled to resemble the depot of its peers right after its node joins the cluster and before running queries.  The depot enables heavy-I/O queries to run several times faster which is important for customers who only have a few hours after midnight everyday to produce heavy-duty reports.

[V] Even though Vertica and Snowflake store a table as a set of separate files of columns in certain sort orders and encodings, Table design in Vertica goes beyond that. A Vertica table is only logical. Its actual physical storage is represented by one or many _**projections**_, each of which can be sorted and encoded differently to support the needs of different queries. The projections are designed automatically by Vertica DBDesigner<sup>(*15) & ($2)</sup> based on specific customer's data, query workload and storage budget. Because there are many projection choices for a table, when a query is issued, Vertica Query Optimizer selects the best set of projections for that query. This is a very unique offer of Vertica that customizes storage design for specific customer workload for query performance and throughput gaining. 

Compute is a key component of the cloud DB architecture and both Snowflake and Vertica have their own advantages.  A compute includes _a set of nodes_ that either run many tasks of a query in parallel or run many queries in parallel or both. A compute in Snowflake is called a _**VW**_ (Virtual Warehouse) and in Vertica is a _**sub-cluster**_, they both can handle isolated workload without talking with their peer VWs or sub-clusters respectively, and hence VWs or sub-clusters can be added or removed as needed into Snowflake or Vertica respectively to achieve linear throughput scaling. This is why both Vertica and Snowflake are famous for their _**elastic throughput scaling**_ that scales the number of queries running in a defined time such as second or minute<sup>(*16)</sup>. So what are the differences between them?
* [S] Since Snowflake splits its operation and execution layers, a node can be added into a VW in a blink while a query is running and can share the workload of that running query immediately to have it finnish sooner. This feature is called _**elastic crunch scaling**_ that scales the runtime of a query by adding/removing nodes. Adding a node in Vertica is a bit slower due to some operation overhead and cannot join the already-started-running queries either because of the way the query plan is built or the way a sub-custer is designed. Vertica, however, does offer elastic crunch scaling automatically when the number of nodes in the sub-cluster is a multiple  of a shard count<sup>(*17)</sup>. 
* [V] While the size of a Vertica sub-cluster can be flexible for elastic crunch scaling, the common practice is to have its size the same as or multiple of its _**number of shards**_ which is a mechanism to define the _**degree of inter-node parallelism**_ in Vertica. Data of all large-size projections are horizontally segmented on a specified column (or a set of columns) into the number of shards. Data of the segmented columns of different tables which are landed on the same shard have the same range of values. If these columns are joined or grouped in a query, the execution will be fast because their data are already co-located on the same node, and no need to transfer data between them<sup>(*18)</sup>. This is another unique design of Vertica that clearly pushes its query performance.
* [S or V?] In summary, both Snowflake and Vertica offer almost linear elastic throughput scaling. Vertica has segment shard setup to push query performance but its elastic crunch scaling has some restrictions. Snowflake has no design of segment shards to push performance for complicated queries but does benefit from immediate crunch scaling. I am curious to see whether the crunch scaling in Snowflake reaches and then passes the performance of Vertica's segment shard setup or it saturates even before hitting that level of performance.

[V] As mentioned earlier, Snowflake offers single mode which is a pure Cloud DB implementing the major technology of separation of storage and compute. Its cloud providers include Amazon AWS, MS Azure, and Google Cloud. Vertica, on the other hand, offers three different DB modes:
1. _**Vertica Eon mode on Cloud**_: this offer is mostly like Snowflake's and the comparisons so far with Snowflake are on this mode. At the time of this writing, this mode only works on Amazon AWS.
2. _**Vertica Eon mode On-Premise**_: This mode also offers the separation of storage and compute technology like the one above but its compute and storage do not have to be on public clouds. The compute can be any cluster customers choose and the storage can be either Pure Storage FlashBlade, MiIO, and HDFS<sup>(*19)</sup>.
3. _**Vertica On-Premise**_: This is the original Shared-Nothing Distributed Data Warehouse of Vertica, which can be installed on a customer's own cluster or any cluster on clouds such as Amazon AWS, MS Azure, and Google Cloud. Note that because this is a shared-nothing cluster, each node has to include both compute (CPU & RAM) and enough disk space for persistent data.

[S] Collaboration between different accounts is one of the key offers of Snowflake conveyed by its  feature, _**Secure Database Sharing**_, through a producer-consumers model without copying data. The producer creates DB Sharing Metadata that includes which to share and who can consume them. Then consumers create corresponding metadata to let snowflake know they want to access the sharable ones. When the protocol is established, the consumers can access and join the shared data with their own. To my knowledge, this feature is not available in Vertica at this time.

[V] Besides integrating with AI (Artificial Intelligence) and ML (Machine Learning) tools, Vertica offers in-db ML in which the ML functions are written in SQL and executed just like a Vertica query but including all ML calculations in the query data pipeline<sup>(*21) & (#4)</sup>. There is no need to export and reimport data anywhere. Snowflake does not have this capability yet but from their CIDR 2021 talk linked above, it seems they are working on delivering similar ones.  

[V] Vertica offers more mature data types such as Complex Types and Time Series (TS), and diverse materialized views such as Flattened Tables (FT) and Live Aggregate Projections (LAP) <sup>(*22)</sup>. Snowflake, however, enables UNDROP a table within 24 hours since it was dropped (by accident).

[S] Snowflake continues scaling their Operation/Cloud Service layer to manage and support their single cluster of unlimited VWs that they call _**Global Data Mesh**_, which supports global communication and moving data in parallel for critical tasks such as cross-region data replication.

Here are a few other differences of their internal implementations:
* [S or V?] In an execution compute of many nodes, many times none-co-located data of a running query with joins and group-by needs to be redistributed between nodes in order to produce the right results. Snowflake lets its execution compute decide when to do that, while Vertica decides that at its planning time before sending it to the execution engine for running. Without knowing the detail of the implementation, it is hard to know which strategy works better.
* [S or V?] The query plan was built in a _**bottom-up**_ fashion in Vertica but _**top-down**_ in Snowflake. They are well-known research techniques that have their own advantages.
* [V] Snowflake uses _**push**_ strategy to pipeline data in a query plan. Vertica had implemented push technique at first but then converted to _**pull**_ strategy to get better performance and resource sharing.
* [S or V?] Snowflake stores its metadata in a key-value store, Foundation DB. Vertica implements its own format and storage.

## Afterward

Now that you know the two DBs' similarities and differences, if you are a customer who is searching for a Cloud DB, which one would best fit your needs? If you are a DB builder, which technologies and offers will you pick for your new DB designs?

## Footnotes & References

* <sup>(1) Vertica and Snowflakes  support defining and maintaining _constraints_ such as PK and FK but only a few of them are enforced automatically. See [Snowflake Constraints](https://docs.snowflake.com/en/sql-reference/constraints.html) & [Vertica Constraints](https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/AdministratorsGuide/Constraints/AboutConstraints.htm?tocpath=Administrator%27s%20Guide%7CConstraints%7C_____0) for how to enforce the rest.</sup>
* <sup>(2) ELT is a new trend to work with semi-structured data that needs to be extracted and then loaded before transformations are applied within the database. It is different from the traditional ETL that transforms data before loading.</sup>
* <sup>(3) Examples of common column encodings include RLE (Run Length Encoding), Delta Value, Delta Range, Block Dictionary. Refer to [Column Store](https://web.archive.org/web/20100619191833/http://db.lcs.mit.edu/projects/cstore/vldb.pdf) and [Vertica](https://vldb.org/pvldb/vol5/p1790_andrewlamb_vldb2012.pdf) for more details.</sup>
* <sup>(*) **Related readings**: (*1) [Column Store technologies](https://web.archive.org/web/20100619191833/http://db.lcs.mit.edu/projects/cstore/vldb.pdf), (*2) Historical Queries of [Snowflake](https://docs.snowflake.com/en/sql-reference/constructs/at-before.html) & [Vertica](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/AnalyzingData/Queries/HistoricalSnapshotQueries.htm?zoom_highlight=Historical%20Queries), (*3) Zero-copy clones in [Snowflake](https://docs.snowflake.com/en/sql-reference/sql/create-clone.html) & [Vertica](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/Functions/VerticaFunctions/COPY_TABLE.htm?zoom_highlight=COPY%20TABLE), (*4) Semi-structured data in [Snowflake](https://docs.snowflake.com/en/user-guide/semistructured-concepts.html) & [Vertica](https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/FlexTables/FlexTableHandbook.htm?tocpath=Using%20Flex%20Tables%7C_____0), (*5) Security in [Snowflake](https://docs.snowflake.com/en/user-guide-admin-security.html) & [Vertica](https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/Security/ImplementingSecurity.htm?tocpath=Security%20and%20Authentication%7C_____0) or [Vertica Voltage](https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/VoltageIntegration/IntegratingWithVoltageSecureData.htm?tocpath=Integrating%20with%20Voltage%20SecureData%7C_____0), (*6) Kafka Data streaming in [Snowflake](https://docs.snowflake.com/en/user-guide/kafka-connector.html) & [Vertica](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/KafkaIntegrationGuide/KafkaIntegrationGuide.htm?tocpath=Integrating%20with%20Apache%20Kafka%7C_____0), (*7) Spark connector in [Snowflake](https://docs.snowflake.com/en/user-guide/spark-connector.html) & [Vertica](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SparkConnector/ApacheSparkGuide.htm?tocpath=Integrating%20with%20Apache%20Spark%7C_____0), (\*8) SDK: [Snowflake Python connector](https://docs.snowflake.com/en/user-guide/python-connector.html) & [Vertica C++ SDK](https://www.vertica.com/docs/10.0.x/HTML/Content/Resources/VerticaResources/CppSDKDoc.htm?tocpath=_____25), [Vertica Java SDK](https://www.vertica.com/docs/10.0.x/HTML/Content/Resources/VerticaResources/JavaSDKDoc.htm?tocpath=_____26), [Vertica Python SDK](https://www.vertica.com/docs/10.0.x/HTML/Content/Resources/VerticaResources/PythonSDKDoc.htm?tocpath=_____27), [Vertica R SDK](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/R-SDK/OverviewRSDK.htm?tocpath=R%20SDK%20API%20Documentation%7C_____0), (\*9) Driver connectors in [Snowflake](https://docs.snowflake.com/en/user-guide/conns-drivers.html) & [Vertica](https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/ConnectingToVertica/ConnectingToVertica.htm?tocpath=Connecting%20to%20Vertica%7C_____0), (\*10) BI partners with [Snowflake](https://docs.snowflake.com/en/user-guide/ecosystem-bi.html#) & [Vertica](https://www.vertica.com/partners/filter/technology-partners/), (\*11) [Vertica Query Optimizer](https://www.researchgate.net/publication/269306314_The_Vertica_Query_Optimizer_The_case_for_specialized_query_optimizers), (\*12) [Vertica SIPS](https://15721.courses.cs.cmu.edu/spring2019/papers/15-execution/shrinivas-icde2013.pdf), (\*13) [Snowflake Cache](https://www.analytics.today/blog/caching-in-snowflake-data-warehouse), (\*14) [Vertica Depot](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/Eon/Depot/DepotManagement.htm?tocpath=Using%20Eon%20Mode%7CDepot%20Management%7C_____0), (\*15) [Vertica DBD](https://ieeexplore.ieee.org/document/6816725), (\*16) Elastic throughput scaling in [Snowflake](https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html) and [Vertica](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/Eon/ImprovingQueryThroughput.htm?tocpath=Using%20Eon%20Mode%7CScaling%20Your%20Eon%20Mode%20Database%7C_____1), (\*17) [Vertica Elastic Crunch Scaling](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/Eon/UsingECS.htm?tocpath=Using%20Eon%20Mode%7CScaling%20Your%20Eon%20Mode%20Database%7C_____2), (\*18) Concepts of Vertica [Identical Segmentation](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/AnalyzingData/Optimizations/AvoidingResegmentationDuringJoins.htm?zoom_highlight=segmentation) based on the [Hash Segmentation Clause](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/Statements/hash-segmentation-clause.htm?zoom_highlight=segmentation) of its [Projection Segmentation](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/ConceptsGuide/Components/ProjectionSegmentation.htm?zoom_highlight=segmentation), (\*19) [Vertica Eon mode On-Premise](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SupportedPlatforms/EonModeCommunalStoragePlatforms.htm), (\*20) [Snowflake Data Sharing](https://docs.snowflake.com/en/user-guide-data-share.html), (\*21) [Vertica in-DB ML](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/AnalyzingData/MachineLearning/MachineLearning.htm?tocpath=Analyzing%20Data%7CMachine%20Learning%20for%20Predictive%20Analytics%7C_____0), (\*22) [Vertica FT, LAP, TS](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/AnalyzingData/AnalyzingData.htm?tocpath=Analyzing%20Data%7C_____0) and [Complex Types](https://www.vertica.com/docs/10.0.x/HTML/Content/Authoring/SQLReferenceManual/DataTypes/ExternalTypes.htm?tocpath=SQL%20Reference%20Manual%7CSQL%20Data%20Types%7CComplex%20Types%7C_____0)  </sup>


* <sup>(#) **Related demos**: (#1) [Snowflake](https://www.youtube.com/watch?v=WU3oGByPatU), (#2) [Vertica's ElasticDW](https://www.youtube.com/watch?v=t5aUxP_eeYk&list=PLQWuZGDiID_4AIogKRqN1GH0HKWa3W-Ci&index=1), (#3) [Snowflake Hot-cache](https://www.youtube.com/watch?v=lcO8CRT5EMc&feature=youtu.be), (#4) [Vertica in-db ML](https://www.youtube.com/watch?v=4JDyvQjCXUY)  </sup>

* <sup> ($) **Related talks**: ($1) [Vertica End-to-End Security](https://www.thecube.net/vertica-bigdata-2020/content/Videos/7zL6tG5oFjM5gesaa), ($2) [Vertica DBD](https://www.thecube.net/vertica-bigdata-2020/content/Videos/Y4ckvNyeoQAZBoBSQ) </sup>